---
layout: post
title:  "Tutoriel Machine Learning sur un cas pratique : les survivants  du Titanic"
date:   2019-06-25
summary: >-
  todo
tags: [machine_learning]
author: Cl√©ment Bosc, C√©dric Lespagnol, Sarrah Khammassi
---

![](/assets/img/ml_dummies/titanic.jpeg)

## Qu'est-ce que le Machine Learning ?

#### Les origines

"Le Machine Learning (ML) a √©merg√© dans la seconde moiti√© du XX√®me si√®cle du domaine de l‚Äôintelligence artificielle et correspond √† l‚Äô√©laboration d‚Äôalgorithmes capables d‚Äôaccumuler de la connaissance et de l‚Äôintelligence √† partir d‚Äôexp√©riences, sans √™tre humainement guid√©s au cours de leur apprentissage, ni explicitement programm√©s pour g√©rer telle ou telle exp√©rience ou donn√©e sp√©cifique."

Ces algorithmes se basent tous ou presque sur des concepts issus du monde des statistiques ou math√©matiques en g√©n√©ral.

De nombreux algorithmes pr√©sent√©s dans ce tutoriel ont √©t√© imagin√©s √† la fin du si√®cle dernier mais ont seulement √©t√© popularis√©s √† partir des ann√©es 2010. Cela est principalement d√ª aux moyens √† disposition des data scientists (puissance de calcul) et de la quantit√© de donn√©es produites en permanence sur le web.

#### M√©thodologie

![](/assets/img/ml_dummies/workflow_small.png)
<br/>

Les principales √©tapes d'un processus classique de Machine Learning sont les suivantes :
* **1. Analyse** : r√©aliser des statistiques descriptives sur les donn√©es, faire quelques hypoth√®ses pour mieux saisir la nature du probl√®me.
* **2. Pr√©paration des donn√©es** : Etape tr√®s importante et d√©terminante pour la r√©ussite du processus complet : il s'agit de s√©lectionner les traits importants ("features") qui permettront d'effectuer une bonne classification par les algos choisis.
* **3. Choix et entrainement des mod√®les** : proc√©d√© it√©ratif qui consiste √† affiner les param√®tres du ou des mod√®les pour obtenir le meilleur r√©sultat possible.
* **4. Evaluation** : test et validation du mod√®le le plus performant.

#### Supervis√© vs Non supervis√©

Il existe deux principales t√¢ches en machine learning : de l‚Äôapprentissage supervis√© et non supervis√©.

* **Apprentissage supervis√©** : nous avons √† notre disposition un jeu de donn√©es labellis√©es, d√©coup√© en trois sous-ensembles avec pour objectif de pr√©dire correctement le label des donn√©es. Ce label peut √™tre une classe dans le cas d‚Äôun probl√®me de classification (spam / non-spam par ex) ou une valeur num√©rique dans le cas  d‚Äôune r√©gression (pr√©dire le prix d‚Äôun appartement par ex).

    Les trois sous-ensembles sont :
    * Un ensemble d‚Äôapprentissage (‚Äútrain‚Äù), qui doit √™tre utilis√© pour cr√©er vos mod√®les d'apprentissage. Pour cet ensemble, une ‚Äúv√©rit√© terrain‚Äù ou ‚Äúground truth‚Äù est fournie, autrement dit, un label ou classe pour chaque ligne. Le mod√®le sera bas√© sur des ¬´caract√©ristiques¬ª (‚Äúfeatures‚Äù), et vous pouvez √©galement utiliser le proc√©d√© de ‚Äúfeature engineering‚Äù pour cr√©er de nouvelles caract√©ristiques (nous d√©taillerons ce proc√©d√© plus tard).
    * Un ensemble de ‚Äúvalidation‚Äù qui doit √™tre utilis√© pour calculer un score pour votre mod√®le en comparant les classes pr√©dites avec la v√©rit√© terrain. Votre mod√®le doit √™tre am√©lior√© it√©rativement jusqu‚Äô√† l‚Äôobtention d‚Äôun score satisfaisant.
    * Un ensemble de ‚Äútest‚Äù, pour lequel on pr√©dit les classes pour obtenir le score final de votre mod√®le. Il ne doit √™tre utilis√© qu‚Äôune fois pour que les donn√©es n‚Äôaient jamais √©t√© vues par votre mod√®le, donc pour que le mod√®le ne soit pas sp√©cialis√© sur cet ensemble pr√©cis (√©viter le ph√©nom√®ne de sur-apprentissage).
* **Apprentissage non supervis√©** : dans ce cas, nous n‚Äôavons pas de donn√©es labellis√©es et l‚Äôobjectif pour le mod√®le est de grouper les donn√©es similaires par lui m√™me. On parle de ‚Äúclustering‚Äù : par exemple regrouper des articles ou des textes portant sur des sujets communs.

En pratique, il est assez long et co√ªteux d‚Äôobtenir des donn√©es labellis√©es : on utilise soit des jeux de donn√©es publiques ou communautaires, soit de l‚Äôapprentissage non supervis√© qui est bien moins pr√©cis que l‚Äôapprentissage supervis√© dans la plupart des cas.

#### Types de caract√©ristiques

Les caract√©ristiques utilis√©es dans les mod√®les de Machine Learning sont des variables que l'on peut classer selon deux grands types :

* Variables num√©riques :
    Ces variables sont des nombres et peuvent √™tre d√©compos√©es en deux sous-ensembles :
    * Variables continues : elles peuvent prendre n‚Äôimporte quelle valeur comprise dans un ensemble de nombres r√©els. (ex : age)
    * Variables discr√®tes : ce sont des valeurs enti√®res, elles ne peuvent pas √™tre une fraction entre une valeur possible et la suivante. (ex : nombre de roues d‚Äôun v√©hicule)
* Variables cat√©gorielles :
    Les valeurs de ces variables sont d√©finies √† partir d‚Äôun petit groupe de cat√©gories possibles. Il existe √©galement deux sous-ensembles de variables :
    * Variables ordinales : les valeurs de ces variables sont des cat√©gories qui peuvent √™tre logiquement ordonn√©es. (ex : faible / moyen / haut)
    * Variables nominales : ce sont des cat√©gories qui ne peuvent pas √™tre organis√©es en s√©quence logique. (ex : masculin / f√©minin)



## ‚ÄúTitanic: Machine Learning from Disaster‚Äù
Pour pr√©senter notre cas pratique, nous nous appuyons sur le Challenge Kaggle [‚ÄúTitanic: Machine Learning from Disaster‚Äù](https://www.kaggle.com/c/titanic).

#### Kaggle ?

Kaggle est une plateforme web organisant des comp√©titions en science des donn√©es. Sur cette plateforme, les entreprises proposent des probl√®mes en science des donn√©es et offrent un prix aux concurrents obtenant les meilleures performances.

#### En quoi consiste le challenge Titanic ?

Le challenge Titanic est un peu le ‚ÄúHelloWorld‚Äù de la Data Science. L‚Äôobjectif est de pr√©dire, √† partir du manifeste des passagers du fameux bateau coul√© en 1912, qui sont ceux qui ont surv√©cu.

Vous avez √† votre disposition deux jeux de donn√©es :
* un ensemble d‚Äôapprentissage (‚Äútrain‚Äù), qui doit √™tre utilis√© pour cr√©er vos mod√®les d'apprentissage. Pour cet ensemble, un label 0 ou 1 pour indiquer si le passager a surv√©cu est fourni. On pourra baser le model sur des ‚Äúfeatures‚Äù telles que le sexe et l‚Äô√¢ge des passagers.
* un ensemble de ‚Äútest‚Äù, qui doit √™tre utilis√© pour voir si votre mod√®le fonctionne correctement. Dans le cadre de ce tutoriel, nous avons cach√© les classes de cet ensemble et proposons un outil de soumission pour √©valuer vos r√©sultats. C'est √† vous de pr√©dire ces r√©sultats !

#### Alors, le challenge du Titanic,  apprentissage supervis√© ou non supervis√© ?

Supervis√© bien s√ªr ! Nous avons un ensemble d'entra√Ænement qui est labellis√© et l‚Äôobjectif est de pr√©dire la classe pour de nouvelles donn√©es non labellis√©es.

#### Pr√©sentation des donn√©es



| Variable 	| D√©finition                                   	| Valeurs possible                               	| Type de donn√©e 	|
|----------	|----------------------------------------------	|------------------------------------------------	|----------------	|
| survival 	| Survival                                     	| 0 = No, 1 = Yes                                	| Classe √† pr√©dire 	|
| pclass   	| Classe du ticket                             	| 1 = 1st, 2 = 2nd, 3 = 3rd                      	| Ordinale       	|
| name     	| Nom du passager                              	|¬†.¬†	|                	|
| sex      	| Sexe                                         	| male, female                                   	| Nominale      	|
| Age      	| Age en ann√©e                                 	| .	| Continue       	|
| sibsp    	| Nombre de fr√®res et soeurs ou conjoints √† bord 	|¬†.¬†	| Discr√®te       	|
| parch    	| Nombre de parents et enfants √† bord          	|¬†.¬†	| Discr√®te       	|
| ticket   	| Num√©ro du ticket                             	|¬†.¬†	|                	|
| fare     	| Tarif passager                               	|¬†.¬†	| Continue       	|
| cabin    	| Num√©ro de cabine                             	|¬†.¬†	|                	|
| embarked 	| Port d'Embarquement                       	| C = Cherbourg, Q = Queenstown, S = Southampton 	| Nominale      	|

## Les choses s√©rieuses commencent

### Quelques imports de base

* **`pandas`** : il s'agit d'une librairie qui permet de manipuler des jeux de donn√©es (appel√©s dataframe)
* **`matplotlib.pyplot`** : permet de faire de la visualisation de donn√©es


{% highlight python %}
import pandas as pd
import matplotlib.pyplot as plt
{% endhighlight %}


Et maintenant nos deux jeux de donn√©es


{% highlight python %}
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
{% endhighlight %}

Voyons voir d'un peu plus pr√®s ce que contient notre ensemble d'entrainement :


{% highlight python %}
train.head()
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



### 1. Observations et statistiques descriptives


{% highlight python %}
train.info()
{% endhighlight %}

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
    PassengerId    891 non-null int64
    Survived       891 non-null int64
    Pclass         891 non-null int64
    Name           891 non-null object
    Sex            891 non-null object
    Age            714 non-null float64
    SibSp          891 non-null int64
    Parch          891 non-null int64
    Ticket         891 non-null object
    Fare           891 non-null float64
    Cabin          204 non-null object
    Embarked       889 non-null object
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.6+ KB


Comme on peut le voir, certaines "features" contiennent des **donn√©es nulles, vides ou manquantes**. C'est la cas de l'age (714 valeurs non nulles sur 891 lignes en tout), du num√©ro de cabine et du pont d'embarquement. Il faudra rem√©dier √† ce probl√®me.

#### Quelle est la distribution des valeurs num√©riques ?


{% highlight python %}
train.describe()
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>446.000000</td>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>257.353842</td>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>223.500000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>446.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>668.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>891.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>



* Le jeu d'entrainement contient 891 entr√©es; soit environ 40% du nombre de passagers r√©els √† bord du Titanic (2224).
* Environ 38% des personnes de notre √©chantillon ont surv√©cu, ce qui est repr√©sentatif car ce nombre est de 32% sur l'ensemble du Titanic.
* La plupart des passagers ( > 75%) n'ont pas voyag√© avec des parents ou des enfants.
* Environ 30 % des passagers ont des fr√®res et soeurs et/ou un.e conjoint.e.
* Il n'y a pas beaucoup de personnes ag√©es et le prix des billets est relativement faible ( au moins 75% ont pay√© moins de 31$ mais certains ont pay√© jusqu'√† 512$).

#### Quelle est la distribution des valeurs cat√©gorielles ?


{% highlight python %}
train.describe(include=['O'])
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Sex</th>
      <th>Ticket</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891</td>
      <td>891</td>
      <td>891</td>
      <td>204</td>
      <td>889</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>891</td>
      <td>2</td>
      <td>681</td>
      <td>147</td>
      <td>3</td>
    </tr>
    <tr>
      <th>top</th>
      <td>Morrow, Mr. Thomas Rowan</td>
      <td>male</td>
      <td>1601</td>
      <td>B96 B98</td>
      <td>S</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1</td>
      <td>577</td>
      <td>7</td>
      <td>4</td>
      <td>644</td>
    </tr>
  </tbody>
</table>
</div>



* Les noms sont uniques dans ce dataset (count=unique=891)
* Il y a plus d'hommes que de femmes (65%)
* Les num√©ros de cabines ont plusieurs doublons, ce qui veut dire que des passagers partagaient une cabine
* La plupart des passagers ont embarqu√© au port S
* Il y a beaucoup de doublons (22%) parmi les num√©ros de ticket (unique=681)

### 2. Nettoyage et feature engineering

#### Suppression des features inutiles

On suppose que l'on peut supprimer le num√©ro de ticket de notre √©chantillon, car il contient beaucoup de doublons et on peut raisonnablement penser qu'il n'y a pas de corr√©lation avec la survie du passager. On peut √©galement supprimer le num√©ro de cabine car il y a beaucoup de valeurs manquantes. La valeur `PassengerId` peut aussi √™tre supprim√©e car celle-ci n'apporte pas d'information. Nous ne la supprimons pas de l'ensemble de test car nous en avons besoin pour la soumission et l'√©valuation des r√©sultats.


{% highlight python %}
train = train.drop(['Ticket', 'Cabin', 'PassengerId'], axis=1)
test = test.drop(['Ticket', 'Cabin'], axis=1)
combine = [train, test]
{% endhighlight %}

#### Cr√©ation d'une nouvelle feature "Title" √† partir du nom du passager

On remarque que les noms des passagers contiennent un titre (Mrs., Mr., etc). Il serait judicieux d'extraire cette information pour en faire une nouvelle feature.


{% highlight python %}
for dataset in combine:
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

pd.crosstab(train['Title'], train['Sex'])
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Sex</th>
      <th>female</th>
      <th>male</th>
    </tr>
    <tr>
      <th>Title</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Capt</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Col</th>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Countess</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Don</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Dr</th>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>Jonkheer</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Lady</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Major</th>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Master</th>
      <td>0</td>
      <td>40</td>
    </tr>
    <tr>
      <th>Miss</th>
      <td>182</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Mlle</th>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Mme</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Mr</th>
      <td>0</td>
      <td>517</td>
    </tr>
    <tr>
      <th>Mrs</th>
      <td>125</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Ms</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Rev</th>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>Sir</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



Nous pouvons regrouper les titres qui apparaissent peu sous une seule cat√©gorie. (`Rare`)


{% highlight python %}
for dataset in combine:
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',
                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')

train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Title</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Master</td>
      <td>0.575000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Miss</td>
      <td>0.702703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Mr</td>
      <td>0.156673</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mrs</td>
      <td>0.793651</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Rare</td>
      <td>0.347826</td>
    </tr>
  </tbody>
</table>
</div>



Nous pouvons maintenant supprimer la colonne `Name` qui est inutile en soit.


{% highlight python %}
train = train.drop(['Name'], axis=1)
test = test.drop(['Name'], axis=1)
combine = [train, test]
{% endhighlight %}

#### Substitution des valeurs manquantes

Comme nous l'avions soulign√© plus t√¥t dans notre analyse, les features `Age`, `Embarked` et `Fare` pr√©sentent des valeurs manquantes. Une technique courante est de remplacer ces valeurs par une valeur par d√©faut.

Pour le port d'embarquement, il y a deux valeurs manquantes et les valeurs possibles sont S, Q et C. On peut simplement remplacer ces valeurs manquantes par la valeur la plus fr√©quente.


{% highlight python %}
freq_port = train.Embarked.dropna().mode()[0]
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)
{% endhighlight %}

Pour l'age et le tarif du ticket, nous proposons de remplacer les valeurs manquantes par la m√©diane.


{% highlight python %}
for dataset in combine:
    dataset['Fare'].fillna(dataset['Fare'].dropna().median(), inplace=True)
    dataset['Age'].fillna(dataset['Age'].dropna().median(), inplace=True)
{% endhighlight %}

#### Conversion des valeurs cat√©gorielles vers nominales

Nous pouvons maintenant convertir les features qui contiennent des chaines de caract√®res en valeurs num√©riques. Cette √©tape est n√©cessaire pour la tr√®s grande majorit√© des algorithmes de Machine Learning.


{% highlight python %}
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for dataset in combine:
    dataset['Sex'] = le.fit_transform(dataset['Sex'])
    dataset['Title'] = le.fit_transform(dataset['Title'])
    dataset['Embarked'] = le.fit_transform(dataset['Embarked'])
{% endhighlight %}

Nous avons utilis√© un `LabelEncoder` qui attribue un chiffre √† chaque valeur unique. Cependant, il est possible  que le mod√®le interpr√®te ces valeurs comme des valeurs ordinales. Il pourra √™tre interessant d'utiliser un `OneHotEncoder`.

Voyons voir √† nouveau notre jeu de donn√©es pr√©par√© et nettoy√©


{% highlight python %}
train.head()
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



Pas mal ! üëå

### 3. Enfin un peu de Machine Learning

Dans ce tutoriel, nous utiliserons la librairie open source `sklearn` (ou `scikit-learn`) qui impl√©mente de tr√®s nombreux algorithmes de Machine Learning.
Dans un 1er temps, nous allons cr√©er nos ensembles train/test/validation. Nous utiliserons le jeu de test fourni et cr√©erons l'ensemble de validation en prenant 20% de l'ensemble de train. Nous utiliserons pour cela la fonction `train_test_split` d√©j√† impl√©ment√©e dans `sklearn`.

Les algorithmes sont tous bas√©s sur la m√™me structure et attendent en entr√©e de la fonction d'apprentissage (`.fit(X_train, y_train)`):
* **X_train**, qui est une matrice ($$nb\_features * nb\_exemples$$)
* **y_train**, qui est un tableau de taille $$nb\_exemples$$ contenant les classes pour chaque exemple

Une fois la phase d'apprentissage termin√©e (tr√®s rapide pour des petits ensembles comme les notres), il convient d'√©valuer le mod√®le construit avec l'ensemble de validation. On peut utiliser une m√©trique √©l√©mentaire, la justesse (`accuracy`) qui consiste √† calculer le ratio $$\frac{nb\_bien\_class√©s}{nb\_total}$$. L'accuracy peut √™tre calcul√©e avec le fonction `.score(X_val, y_val)` du mod√®le.

Enfin, une fois que le mod√®le construit est satisfaisant, on peut pr√©dire les classes de notre ensemble de test pour une √©valuation non biais√©e (afin d'√©viter le ph√©nom√®ne de sur-apprentissage). Pour cela on utilise la fonction `.predict(X_test)`.


{% highlight python %}
from sklearn.model_selection import train_test_split

X_train = train.drop("Survived", axis=1)
y_train = train["Survived"]
X_test  = test.drop("PassengerId", axis=1).copy()

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)

X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape
{% endhighlight %}




    ((712, 8), (712,), (179, 8), (179,), (418, 8))



#### KNeighborsClassifier

L‚Äôalgorithme des KNN ("K plus proches voisins" en fran√ßais) attribue √† un exemple la classe majoritaire parmi ses K plus proches voisins. K est un param√®tre √† optimiser par l‚Äôutilisateur, il est pr√©f√©rable de choisir un K impair pour de la classification binaire pour √©viter les √©galit√©s. On utilise commun√©ment la distance euclidienne entre deux vecteurs de features pour calculer la proximit√© entre 2 exemples.

La phase d‚Äôapprentissage consiste ici seulement √† stocker les vecteurs de features et les labels associ√©s.

Un probl√®me avec le "vote majoritaire" appara√Æt lorsque les classes ont une r√©partition in√©gale. En effet, si une classe est sur-repr√©sent√©e, elle pourra √™tre plus repr√©sent√©e dans les plus proches voisins. Une m√©thode pour contrer ce probl√®me est d‚Äôattribuer un poids plus important aux voisins les plus proches (param√®tre `weight='distance'` dans l‚Äôimpl√©mentation de sklearn).

Voici un sch√©ma pour illustrer un cas o√π K = 5 :

![](/assets/img/ml_dummies/knn.png)


{% highlight python %}
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, y_train)
acc_knn = round(knn.score(X_val, y_val) * 100, 2)
acc_knn
{% endhighlight %}




    74.3



#### DecisionTreeClassifier

Pour faire une pr√©diction, les Decision Trees utilisent un ensemble de r√®gles de d√©cision ¬´ If Then Else ¬ª sur les features. Cette m√©thode permet de d√©composer un ensemble de donn√©es en sous-ensembles de plus en plus petits. On peut ainsi assigner aux sous-ensemble finaux une classe (0 ou 1 pour une classification binaire). Le but du mod√®le va √™tre de cr√©er des sous-ensembles homog√®nes (contenant des exemples de m√™me classe) pour minimiser l‚Äôerreur de ses pr√©dictions.


{% highlight python %}
from sklearn.tree import DecisionTreeClassifier
decision_tree = DecisionTreeClassifier(max_depth=3)
decision_tree.fit(X_train, y_train)
acc_decision_tree = round(decision_tree.score(X_val, y_val) * 100, 2)
print(acc_decision_tree)

import pydotplus
from sklearn import tree
from IPython.display import Image  

# Create DOT data
dot_data = tree.export_graphviz(decision_tree, out_file=None,
                                feature_names=train.columns[1:],
                                class_names=["dead", "alive"])
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  
#Show graph
Image(graph.create_png(), width=800)
{% endhighlight %}

    81.01





![](/assets/img/ml_dummies/decision_tree_output.png)



#### Random Forest

Les mod√®les Random Forest sont compos√©s de plusieurs Decision Trees ("for√™t" d'arbres de d√©cision). Le r√©sultat retourn√© par un Random Forest est la classe majoritaire retourn√©e par les Decision Trees qui le composent. Ces Decision Trees sont chacuns entra√Æn√©s avec un √©chantillon des donn√©es d‚Äôentra√Ænement pris au hasard et un sous-ensemble des features pris au hasard √©galement.

![](/assets/img/ml_dummies/random_forest.png)


{% highlight python %}
from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators=100, max_depth=3)
random_forest.fit(X_train, y_train)
acc_random_forest = round(random_forest.score(X_val, y_val) * 100, 2)
acc_random_forest
{% endhighlight %}




    80.45



#### Logistic Regression

La r√©gression logistique fut l'un des premiers algorithmes de Machine Learning (premi√®re mention en 1944). C'est un algorithme qui est utilis√© quand la classe √† pr√©dire est cat√©gorielle. L'objectif est de s√©parer l'espace vectoriel en deux "r√©gions", une pour chaque classe, par une fronti√®re lin√©aire. Par exemple pour deux dimensions, la fronti√®re est une droite; pour 3 dimensions, la fronti√®re est un plan, etc...  

Pour que cela ait du sens, il faut bien-s√ªr que les points de l'ensemble soient **lin√©airement s√©parables** (ce qui n'est pas forcement facile √† savoir en pratique).

![](/assets/img/ml_dummies/linear_non_linear.PNG)
<br/>
Dans notre cas, (deux classes) on parle de **Binomial Logistic Regression**. Dans les cas o√π l'on a plus de classes, on parle de **Multinomial Logistic Regression**.


{% highlight python %}
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(solver='lbfgs')
logreg.fit(X_train, y_train)
acc_log = round(logreg.score(X_val, y_val) * 100, 2)
acc_log
{% endhighlight %}




    79.89



#### Gaussian Naive Bayes

Les algorithmes de type Naive Bayes se basent sur le [th√©or√®me de Bayes](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Bayes) :  $$P(c \vert e) = \frac{P(e \vert c) * P(c)}{P(e)}$$

Avec :
* $$P(c \vert e)$$ = probabilit√© d‚Äôassigner la classe $$c$$ sachant l‚Äôexemple $$e$$
* $$P(e \vert c)$$ = probabilit√© d‚Äôavoir l‚Äôexemple $$e$$ sachant qu‚Äôon a la classe $$c$$
* $$P(c)$$ = probabilit√© de la classe $$c$$ (nombre d‚Äôexemples de classe $$c$$ / nombre d‚Äôexemples)
* $$P(e)$$ = probabilit√© de l‚Äôexemple $$e$$ (nombre d‚Äôexemples $$e$$ / nombre d‚Äôexemples)

La classe $$c$$ assign√©e √† l‚Äôexemple $$e$$ sera la classe pour laquelle $$P(c \vert e)$$ est maximum. Si l‚Äôon souhaite seulement obtenir une assignation, on peut enlever $$P(e)$$ du calcul qui ne sert qu‚Äô√† normaliser.

Un exemple $$e$$ est repr√©sent√© par ses features $$f_1$$, $$f_2$$, $$f_n$$... La probabilit√© $$P(e \vert c)$$ est donc $$P(f_1, f_2, ..., f_n  \vert c)$$. Les algorithmes Naive Bayes supposent que les features n‚Äôont pas de liens entre elles donc $$P(f_1, f_2, ..., f_n  \vert c) = P(f_1 \vert c) * P(f_2 \vert c) * ... * P(f_n \vert c)$$.

Un mod√®le Naive Bayes stocke donc les probabilit√©s d‚Äôapparition de chaque valeur pour chaque feature et chaque classe en se basant sur leur fr√©quence d‚Äôapparition. Nous utilisons ici la variante Gaussienne qui n‚Äôutilise pas les fr√©quences d‚Äôapparition mais la densit√© de probabilit√© de la [loi normale](https://fr.wikipedia.org/wiki/Loi_normale), donc elle doit stocker la moyenne et l‚Äô√©cart-type de chaque feature sachant chaque classe.


{% highlight python %}
from sklearn.naive_bayes import GaussianNB
gaussian = GaussianNB()
gaussian.fit(X_train, y_train)
acc_gaussian = round(gaussian.score(X_val, y_val) * 100, 2)
acc_gaussian
{% endhighlight %}




    76.54



#### Support Vector Machines (SVM)

L'agorithme de Support Vector Machines (plus commun√©ment appel√©s SVM ou "machines √† vecteurs de support") est un algorithme supervis√© utilis√© pour la classification.

Cet algo a pour but de s√©parer des groupes de donn√©es diff√©rents avec des **hyperplans**. Un hyperplan peut √™tre vu comme une ligne qui s√©pare et classifie lin√©airement un jeu de donn√©es.

Le SVM est capable de r√©aliser 2 types de s√©paration :
* **lin√©aire** : quand les donn√©es sont lin√©airement s√©parables.
* **non lin√©aire** : on utilise un "noyau" (kernel) qui va transformer les features existantes et en cr√©er de nouvelles afin de pouvoir repr√©senter les donn√©es de mani√®re lin√©airement s√©parable. Cette op√©ration est transparente pour l'utilisateur. Il existe plusieurs types de noyaux en fonction de la r√©partition spatiale des donn√©es : Noyau lin√©aire, polynomial, sigmo√Øde, "Radial Basic Function" (RBF)...

<br/>
![](/assets/img/ml_dummies/svm.png)
<br/>


{% highlight python %}
from sklearn.svm import SVC

svc = SVC(gamma='auto')
svc.fit(X_train, y_train)
acc_svc = round(svc.score(X_val, y_val) * 100, 2)
acc_svc
{% endhighlight %}




    73.18



#### Comparaison des mod√®les

Bon ! Nous avons test√© les grands classiques des mod√®les de machine learning, lequel s'en sort le mieux ?


{% highlight python %}
models = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',
              'Random Forest', 'Naive Bayes', 'Decision Tree'],
    'Score': [acc_svc, acc_knn, acc_log,
              acc_random_forest, acc_gaussian, acc_decision_tree]})
models.sort_values(by='Score', ascending=False)
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>Decision Tree</td>
      <td>81.01</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Random Forest</td>
      <td>80.45</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logistic Regression</td>
      <td>79.89</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Naive Bayes</td>
      <td>76.54</td>
    </tr>
    <tr>
      <th>1</th>
      <td>KNN</td>
      <td>74.30</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Support Vector Machines</td>
      <td>73.18</td>
    </tr>
  </tbody>
</table>
</div>



## Evaluation finale!


{% highlight python %}
# Pr√©diction de la classe pour l'ensemble de test
y_pred = decision_tree.predict(X_test)

# Cr√©ation d'un dataframe de submission avec une colonne PassengerId et une colonne Survived
submission = pd.DataFrame({
        "PassengerId": test["PassengerId"],
        "Survived": y_pred
    })

# Enregistrement au format csv
submission.to_csv('output/submission.csv', index=False)
{% endhighlight %}

Et maintenant, v√©rifions l'accuracy de notre mod√®le sur l'ensemble de test


{% highlight python %}
from toolbox import submit, confusion_matrix
final_score = submit('output/submission.csv')
{% endhighlight %}

    Votre accuracy score est de 0.7751196172248804.
    Pas mal, c'est un bon d√©but ! Tu peux encore faire un peu mieux üôÇ


### Matrice de confusion

Pour visualiser les pr√©dictions de notre mod√®le, nous pouvons afficher la **matrice de confusion**, qui indique comment sont class√©es les valeurs pr√©dites par rapport aux valeurs r√©elles. Cette matrice nous apporte des pr√©cisions par rapport au calcul de l'accuracy.

![](/assets/img/ml_dummies/confusion_matrix.png)

On y retrouve des informations comme le nombre de TP (True Positive), FP (False Positive), FN (False Negative) et TN (True Negative), √† partir desquelles ont peut calculer le **rappel**, la **pr√©cision** et l'**accuracy**.


{% highlight python %}
pd.DataFrame(confusion_matrix(y_pred), index = ["Predicted survived", "Predicted dead"], columns=["Really survived", "Really dead"])
{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Really survived</th>
      <th>Really dead</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Predicted survived</th>
      <td>212</td>
      <td>48</td>
    </tr>
    <tr>
      <th>Predicted dead</th>
      <td>46</td>
      <td>112</td>
    </tr>
  </tbody>
</table>
</div>



## Et maintenant, c'est √† vous !

Ce tutoriel pr√©sente les principaux algorithmes de Machine Learning. Nous avons construit un mod√®le se basant sur un arbre de d√©cision et nous avons obtenu un score de 0.775 dans la phase d'√©valuation finale sur notre ensemble de test. Mais √ßa ne veut pas dire que l'on ne peut pas faire mieux, bien au contraire !!
Maintenant, c'est √† vous de jouer, l'objectif est d'am√©liorer le score.

Pour cela voici quelques pistes :
* Tester plusieurs valeurs diff√©rentes pour les hyperparam√®tres des mod√®les pr√©sent√©s. Pour ce tutoriel nous avions utilis√© les valeurs par d√©faut mais il existe d'autres param√®tres qui, selon les cas, peuvent am√©liorer l'accuracy. La fonction `sklearn.model_selection.GridSearchCV` pourra vous y aider.
* Tester d'autres mod√®les non pr√©sent√©s ici (voir liste compl√®te sur [scikit-learn.org](https://scikit-learn.org/)).
* Cr√©er de nouvelles features √† partir de celles existantes : voyage_seul (binaire) si  `SibSp > 1 OU Parch > 1`, grouper les ages ou les prix par tranches (voir la fonction `pd.cut(train['Age'], nb_bands)`) ... √† vous d'√™tre inventifs üòÅ.
* Utiliser un `OneHotEncoder` √† la place d'un `LabelEncoder`.
* Utiliser une k-fold Cross-Validation (√ßa ne permettra pas d'am√©liorer votre score mais c'est une bonne pratique √† connaitre)


#### R√©f√©rences

* [Challenge Kaggle "Titanic: Machine Learning from Disaster"][titanic_kaggle]
* [Titanic Data Science Solutions][kaggle_kernel_sol]
* [Understanding Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)
* [Une petite histoire du Machine Learning](https://www.quantmetry.com/une-petite-histoire-du-machine-learning)

[titanic_kaggle]: https://www.kaggle.com/c/titanic
[kaggle_kernel_sol]: https://www.kaggle.com/startupsci/titanic-data-science-solutions
